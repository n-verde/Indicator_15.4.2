{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "This notebook is used for mapping land cover, at a national scale for Greece, using Sentinel-2 composites (with 4 bands) and a weakly supervised training set from CORINE Land Cover data. Preprocessing of satellite data and reference samples is performed in Google Earth Engine and locally. The model used is the Fine Grained UNET model by  Stoian et al., 2019 (https://www.mdpi.com/2072-4292/11/17/1986), and a weighted categorical crossentropy loss.\n",
        "\n",
        "*creator: Natalia Verde, AUTH, 2023*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zK38zqOCv7Z"
      },
      "source": [
        "## Package versions\n",
        "\n",
        "\n",
        "* Tensorflow version: 2.11.0\n",
        "* Keras version: 2.11.0\n",
        "* h5py version: 3.1.0\n",
        "* Folium version: 0.12.1.post1\n",
        "* Matplotlib version: 3.2.2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkZqGmLZhwyn"
      },
      "source": [
        "## View resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czHuFzoIhz0v"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShfwTlmIh2Cz"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# 2. Libraries, imports and authentications\n",
        "\n",
        "Authenticate and import as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XrAU3TZ6MJc"
      },
      "outputs": [],
      "source": [
        "print('Authenticate and initialize Colab ...')\n",
        "\n",
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jat01FEoUMqg",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "print('Authenticate and initialize Earth Engine ...')\n",
        "\n",
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "\n",
        "try:\n",
        "  import ee\n",
        "  ee.Initialize()\n",
        "except:\n",
        "  ee.Authenticate() # In colab only run once per week - not needed for runing in Jupyter\n",
        "finally:\n",
        "  import ee\n",
        "  ee.Initialize()\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BQg778eCv7x"
      },
      "source": [
        "Connect to Google Drive\n",
        "(not used when running code locally in jupyter)\n",
        "\n",
        "If you have exported the samples in Google Drive, access them from there.\n",
        "\n",
        "see: https://stackoverflow.com/questions/57419346/how-can-i-access-my-google-drive-files-from-google-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0xb3e4FCv71",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Connecting to drive ...\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RnZzcYhcpsQ",
        "outputId": "afe15ef2-2773-40f0-8f3d-f092652154e2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temsorflow version:\n",
            "2.12.0\n",
            "Keras version:\n",
            "2.12.0\n",
            "h5py version:\n",
            "3.8.0\n",
            "Folium version:\n",
            "0.14.0\n",
            "Matplotlib version:\n",
            "3.7.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from google.cloud import storage\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Temsorflow version:\")\n",
        "print(tf.__version__)\n",
        "\n",
        "import keras\n",
        "print(\"Keras version:\")\n",
        "print(keras.__version__)\n",
        "\n",
        "import h5py\n",
        "print(\"h5py version:\")\n",
        "print(h5py.__version__)\n",
        "\n",
        "import folium\n",
        "print(\"Folium version:\")\n",
        "print(folium.__version__)\n",
        "\n",
        "import matplotlib\n",
        "print(\"Matplotlib version:\")\n",
        "print(matplotlib.__version__)\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "# 3. Set global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psz7wJKalaoj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Setting global variables ...\")\n",
        "\n",
        "# Specify names locations for outputs.\n",
        "\n",
        "BASE_FOLDER = \"/content/drive/My Drive/\"  ########## FOR RUNNING WITH GOOGLE DRIVE ##########\n",
        "BUCKET = \"n-verde_bucket_1542\"  ########## FOR RUNNING WITH GOOGLE CLOUD ##########\n",
        "FOLDER = \"1542_CLC_new\"\n",
        "\n",
        "# storing in Drive or Google Cloud?\n",
        "# STORAGE = 'DRIVE'\n",
        "STORAGE = 'GC'\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# ---------------------- FEATURES --------------------------\n",
        "\n",
        "# Specify inputs (Composite bands) to the model and the response variable.\n",
        "\n",
        "otherData = [\n",
        "              # 'DEM',\n",
        "              'D0501NDVI', 'D0801NDVI', # 'D1001NDVI'\n",
        "             ]\n",
        "\n",
        "BANDS = [\n",
        "          'D0501B2', 'D0501B3', 'D0501B4', 'D0501B8',\n",
        "          'D0801B2', 'D0801B3', 'D0801B4', 'D0801B8',\n",
        "          # 'D1001B2', 'D1001B3', 'D1001B4', 'D1001B8'\n",
        "          ] + otherData\n",
        "\n",
        "RESPONSE = ['gt']\n",
        "\n",
        "CLASS_NUMBER = 6 # 1.Forest, 2.Grassland, 3.Cropland, 4.Wetland, 5.Settlement, 6.Other (+ 7.Background/NULL)\n",
        "\n",
        "# Specify the number of tiles the imagery will be split in\n",
        "# Predictions will run per tile\n",
        "S = 10 # split to SxS tiles\n",
        "# S = 100 # split to SxS tiles\n",
        "\n",
        "# pixel size of image for export, training & prediction (in meters)\n",
        "pixelSize = 20\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# ---------------------- SAMPLES --------------------------\n",
        "\n",
        "TRAINING_BASE = 'tra'\n",
        "VAL_BASE = 'val'\n",
        "TEST_BASE = 'test'\n",
        "\n",
        "TRAIN_SIZE =  10000    # 6 samples per polygon * 5 augmentations = 7000 * 5 = 35,000\n",
        "VAL_SIZE =    10000    # 6 samples per polygon * 5 augmentations = 7000 * 5 = 35,000\n",
        "TEST_SIZE =   2604    # 6 samples per polygon # LIFE\n",
        "\n",
        "# BATCH_SIZE is dependent on your GPU memory. (e.g. on my PC it can't be larger than 1, on Colab it can be 4)\n",
        "BATCH_SIZE = 8\n",
        "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
        "EPOCHS = 50 # (Stoian)\n",
        "\n",
        "BUFFER_SIZE = 1500\n",
        "\n",
        "# ---------------------- MODEL -------------------------\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in BANDS\n",
        "]\n",
        "\n",
        "BANDS_DICT = dict(zip(BANDS, COLUMNS))\n",
        "\n",
        "RESPONSE_DICT = dict(zip(\n",
        "      RESPONSE,\n",
        "      [tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in RESPONSE]\n",
        "))\n",
        "# UNIQUES = np.array([0.   , 0.166, 0.333, 0.499, 0.666, 0.833, 1.   ], dtype=\"float32\") # CLASS_NUMBER in one-hot (7)\n",
        "UNIQUES = np.array([0.166, 0.333, 0.499, 0.666, 0.833, 1.   ], dtype=\"float32\") # CLASS_NUMBER in one-hot (6)\n",
        "\n",
        "lr = 0.0001 # initial learning rate (Scepanovic, Schmitt, Stoian 0.001)\n",
        "\n",
        "# ---------- WEIGHTS FOR WEIGHTED CATEGORICAL CROSSENTROPY LOSS\n",
        "# weights: numpy array of shape (C,) where C is the number of classes\n",
        "# Usage: weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "# weights are the 100/percentage of class area in the samples...\n",
        "weights = [\n",
        "\t\t\t\t\t\t1.39, # Forest\n",
        "\t\t\t\t\t\t1.26, # Grassland\n",
        "\t\t\t\t\t\t1.43, # Cropland\n",
        "\t\t\t\t\t\t80.55, # Wetland\n",
        "\t\t\t\t\t\t12.99, # Settlement\n",
        "\t\t\t\t\t\t2.38  # Other\n",
        "\t\t\t\t\t]\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmQiWkk_WhVn"
      },
      "outputs": [],
      "source": [
        "# # #############  FOR TESTING ONLY #########################\n",
        "\n",
        "# TRAIN_SIZE =  50\n",
        "# VAL_SIZE =    25\n",
        "# TEST_SIZE =   25\n",
        "# BUFFER_SIZE = 100\n",
        "\n",
        "# EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3GbA-44E1of"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# ---------------------- GEE (images, ground truth) --------------------------\n",
        "\n",
        "# composite images to use for training and prediction\n",
        "spring = ee.Image('users/n-verde/PhD_1542/MedianMonths5-6_GR_2018')\n",
        "summer = ee.Image('users/n-verde/PhD_1542/MedianMonths7-9_GR_2018')\n",
        "autumn = ee.Image('users/n-verde/PhD_1542/MedianMonths10-11_GR_2018')\n",
        "\n",
        "# mountains\n",
        "kapos = ee.Image('users/n-verde/PhD_1542/Kapos_K1_GR_binary')\n",
        "\n",
        "# DEM\n",
        "dem = ee.Image('users/n-verde/shared/LIFE-IP_4_NATURA/eudem_v11_GR_EPSG4326')\n",
        "\n",
        "# ground truth\n",
        "gt_tra = ee.Image('users/n-verde/PhD_1542/NEW_CLC2018_GR_4326_IPCC_one-hot_TRAIN_14perc')\n",
        "gt_val = ee.Image('users/n-verde/PhD_1542/NEW_CLC2018_GR_4326_IPCC_one-hot_VAL_14perc')\n",
        "gt_test = ee.Image('users/n-verde/PhD_1542/NEW_LIFE_4326_IPCC_one-hot_TEST_30perc') # LIFE\n",
        "\n",
        "# ground truth BOXES VECTOR\n",
        "trainingPolys = ee.FeatureCollection('users/n-verde/PhD_1542/NEW_Kapos_grid_5120m_4326_TRAIN_14perc')\n",
        "valPolys = ee.FeatureCollection('users/n-verde/PhD_1542/NEW_Kapos_grid_5120m_4326_VAL_14perc')\n",
        "testPolys = ee.FeatureCollection('users/n-verde/PhD_1542/NEW_LIFE_grid_TEST_30perc_random')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# 4. Imagery\n",
        "\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).\n",
        "Also use some pre-made geometries to sample the stack in strategic locations. Specifically, these are polygons in which to take the 256x256 samples. Display the sampling polygons on a map, red for training polygons, blue for validation, green for test polygons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7DafSRr8pdb"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IlgXu-vcUEY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Use folium to visualize the imagery.\n",
        "map = folium.Map(location=[37.981892554434936, 23.7269115882649])\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "# rename bands to display\n",
        "renamedOpticalBands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7',\n",
        "                'B8', 'B8A', 'B11', 'B12']\n",
        "\n",
        "bandNames = summer.bandNames()\n",
        "mos79 = summer.select(bandNames, renamedOpticalBands)\n",
        "\n",
        "mapid = mos79.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 300, 'max': 2500})\n",
        "map = folium.Map(location=[37.981892554434936, 23.7269115882649])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='MedianMonths7-9',\n",
        "  ).add_to(map)\n",
        "\n",
        "# ------------------------------------------\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(valPolys, 2).paint(testPolys, 3)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "# display the sample boxes\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 3, 'palette': ['red', 'blue', 'green']})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training & test polygons',\n",
        "  ).add_to(map)\n",
        "\n",
        "# ------------------------------------------\n",
        "# ground truth\n",
        "gt = ee.ImageCollection([gt_tra, gt_val, gt_test]).mosaic()\n",
        "gt = gt.rename('gt')\n",
        "\n",
        "mapid = gt.getMapId({'bands': ['gt'], 'min': 0, 'max': 1})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='ground_truth',\n",
        "  ).add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNGC_Ku92rc7"
      },
      "source": [
        "## Spectral indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb1kjmEg2vUL"
      },
      "outputs": [],
      "source": [
        "ndvi56 = spring.normalizedDifference(['D0501B8', 'D0501B4']).rename('D0501NDVI')\n",
        "ndvi79 = summer.normalizedDifference(['D0801B8', 'D0801B4']).rename('D0801NDVI')\n",
        "# ndvi1011 = autumn.normalizedDifference(['D1001B8', 'D1001B4']).rename('D1001NDVI')\n",
        "# print(ndvi56.getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnzaRN3m8uTG"
      },
      "source": [
        "## Temporal composite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua9np4ps8xoi"
      },
      "outputs": [],
      "source": [
        "mos56 = spring.select(['D0501B2', 'D0501B3', 'D0501B4', 'D0501B8']).addBands(ndvi56)\n",
        "mos79 = summer.select(['D0801B2', 'D0801B3', 'D0801B4', 'D0801B8']).addBands(ndvi79)\n",
        "# mos1011 = autumn.select(['D1001B2', 'D1001B3', 'D1001B4', 'D1001B8']).addBands(ndvi1011)\n",
        "\n",
        "mos = mos56.addBands(mos79) #.addBands(mos1011)\n",
        "# print(mos.getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKp4AobcDx-i"
      },
      "outputs": [],
      "source": [
        "dem = dem.rename('DEM')\n",
        "# print(dem.getInfo())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_KQLZewLAd8"
      },
      "source": [
        "## Normalize\n",
        "Rescale imagery to [0-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmJZG6NlJUNs"
      },
      "outputs": [],
      "source": [
        "############### HELPER FUNCTIONS ###############\n",
        "\n",
        "def findMin(imgBand, bandName) :\n",
        "     MIN = ee.Number(imgBand.reduceRegion(**{\n",
        "    'reducer': ee.Reducer.min(),\n",
        "    'geometry': aoi,\n",
        "    'scale': (pixelSize*2),\n",
        "    'tileScale': 4,\n",
        "    # 'bestEffort': true,\n",
        "    'maxPixels': 1e9\n",
        "    }).get(bandName))\n",
        "\n",
        "     return MIN\n",
        "\n",
        "\n",
        "def findMax(imgBand, bandName) :\n",
        "     MAX = ee.Number(imgBand.reduceRegion(**{\n",
        "    'reducer': ee.Reducer.max(),\n",
        "    'geometry': aoi,\n",
        "    'scale': (pixelSize*2),\n",
        "    'tileScale': 4,\n",
        "    # 'bestEffort': true,\n",
        "    'maxPixels': 1e9\n",
        "    }).get(bandName))\n",
        "\n",
        "     return MAX\n",
        "\n",
        "\n",
        "def rescaleFixedMinMax(img, bandName, MIN, MAX):\n",
        "     Min = ee.Number(MIN)\n",
        "     Max = ee.Number(MAX)\n",
        "\n",
        "     imgBand = img.select(bandName)\n",
        "     imgBand = imgBand.float()\n",
        "     imgBandNorm = imgBand.divide(Max) # normalize the data to 0 - 1\n",
        "\n",
        "     return imgBandNorm\n",
        "\n",
        "\n",
        "def rescaleBand(oneBandImg): # only works for an image with 1 band\n",
        "\n",
        "   bandName = ee.String(oneBandImg.bandNames().get(0)) #print(bandName)\n",
        "\n",
        "   imgBand = oneBandImg.select(bandName)\n",
        "\n",
        "   MIN = findMin(imgBand, bandName) # print('old min', MIN.getInfo())\n",
        "   MAX = findMax(imgBand, bandName) # print('old max', MAX.getInfo())\n",
        "\n",
        "   test = ee.Number(ee.Algorithms.If(MIN.lt(ee.Number(0)), 1, 0)) #print(test)\n",
        "\n",
        "   if (test.eq(ee.Number(1))) :\n",
        "      imgBand = imgBand.add(MIN.abs()) # turn negative values to possitive\n",
        "      MAX = MAX.add(MIN.abs()) # change new max\n",
        "      MIN = ee.Number(0) # change min to 0\n",
        "\n",
        "   min = MIN # print(ee.String(\"min of \").cat(bandName), min)\n",
        "   max = MAX # print(ee.String(\"max of \").cat(bandName), max)\n",
        "\n",
        "  # Apply the rescale def to all bands of the oneBandImg\n",
        "   res = rescaleFixedMinMax(oneBandImg,bandName, min, max) #print(ee.Image(res))\n",
        "\n",
        "   return ee.Image(res)\n",
        "\n",
        "\n",
        "def bandsToCollection(image):\n",
        "   collection = ee.ImageCollection.fromImages(image.bandNames().map(\n",
        "       lambda bandName: image.select(ee.String(bandName))\n",
        "      ))\n",
        "\n",
        "   return collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKkd5osJSH72"
      },
      "outputs": [],
      "source": [
        "############### NORMALIZE ###############\n",
        "\n",
        "print(\"Rescaling optical imagery to 0-1 ...\")\n",
        "\n",
        "# define an aoi in which to compute min and max of image\n",
        "aoi = ee.Image('users/n-verde/auth/PhenologyPaper/AMATH_buff100_raster_compressed').geometry()\n",
        "\n",
        "# turn image to IC in order to .map\n",
        "bandsInFormOfIC = bandsToCollection(mos)\n",
        "\n",
        "# normalize to 0-1\n",
        "rescaledBandsIC = ee.ImageCollection(bandsInFormOfIC.map(rescaleBand))\n",
        "\n",
        "# merge the collection to a single image again\n",
        "def col2img(image, previous):\n",
        "  return ee.Image(previous).addBands(ee.Image(image))\n",
        "\n",
        "rescaledBands = ee.Image(rescaledBandsIC.iterate(col2img, ee.Image()))\n",
        "\n",
        "print(\"Rescaling DEM to 0-1 ...\")\n",
        "\n",
        "# turn image to IC in order to .map\n",
        "bandsInFormOfIC = bandsToCollection(dem)\n",
        "\n",
        "# normalize to 0-1\n",
        "rescaledDEMIC = ee.ImageCollection(bandsInFormOfIC.map(rescaleBand))\n",
        "\n",
        "# merge the collection to a single image again\n",
        "def col2img(image, previous):\n",
        "  return ee.Image(previous).addBands(ee.Image(image))\n",
        "\n",
        "rescaledDEM = ee.Image(rescaledDEMIC.iterate(col2img, ee.Image()))\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndsBPWFNV_hE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "image_res = rescaledBands.addBands(rescaledDEM)\n",
        "\n",
        "# display rescaled image\n",
        "mapid = image_res.getMapId({'bands': ['D0801NDVI'], 'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[37.981892554434936, 23.7269115882649])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='rescaled',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tza7Ooasc-cz"
      },
      "source": [
        "## Mask to  samples extent\n",
        "\n",
        "Mask the rescaled composite image to the samples extent (boxes image).\n",
        "Sample boxes were created based on:\n",
        "* https://www.tandfonline.com/doi/full/10.1080/19475683.2020.1803402\n",
        "* https://explore.openaire.eu/search/publication?pid=10.1109%2Fjstars.2021.3116094\n",
        "* https://www.mdpi.com/2072-4292/10/8/1214\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCL2d23oc_U5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "boxes = ee.ImageCollection.fromImages([gt_tra, gt_val, gt_test]).mosaic().gte(0)\n",
        "masked_image = image_res.mask(boxes)\n",
        "\n",
        "# display the masked image\n",
        "mapid = masked_image.getMapId({'bands': ['D0801B4', 'D0801B3', 'D0801B2'], 'min': 0, 'max': 0.25})\n",
        "map = folium.Map(location=[37.981892554434936, 23.7269115882649])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='mos_masked',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M65JfhsuCv8r"
      },
      "source": [
        "## Split image to tiles\n",
        "Define a function for displaying Earth Engine image tiles on a folium map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7A358m_Cv8t",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define a method for displaying Earth Engine image tiles on a folium map.\n",
        "def add_ee_layer(self, ee_object, vis_params, name):\n",
        "\n",
        "    try:\n",
        "        # display ee.Image()\n",
        "        if isinstance(ee_object, ee.image.Image):\n",
        "            map_id_dict = ee.Image(ee_object).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "            ).add_to(self)\n",
        "        # display ee.ImageCollection()\n",
        "        elif isinstance(ee_object, ee.imagecollection.ImageCollection):\n",
        "            ee_object_new = ee_object.mosaic()\n",
        "            map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "            ).add_to(self)\n",
        "        # display ee.Geometry()\n",
        "        elif isinstance(ee_object, ee.geometry.Geometry):\n",
        "            folium.GeoJson(\n",
        "            data = ee_object.getInfo(),\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "        ).add_to(self)\n",
        "        # display ee.FeatureCollection()\n",
        "        elif isinstance(ee_object, ee.featurecollection.FeatureCollection):\n",
        "            ee_object_new = ee.Image().paint(ee_object, 0, 2)\n",
        "            map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "        ).add_to(self)\n",
        "\n",
        "    except:\n",
        "        print(\"Could not display {}\".format(name))\n",
        "\n",
        "# Add EE drawing method to folium.\n",
        "folium.Map.add_ee_layer = add_ee_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB8HVdLzCv8x",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# This function splits a geometry into equal-sized subrectangles.\n",
        "# splits a geometry to parts^2 subregions\n",
        "# Assumes that the geometry has only 4 vertices.\n",
        "# returns a list of polygons (you can map over it)\n",
        "def split(geom, nSplits):\n",
        "\n",
        "  one = ee.Number(1)\n",
        "\n",
        "  # Return (nSplit+1)^2 coordinate pairs, given 4 vertices.\n",
        "  def toPts(rect, nSplits):\n",
        "    k = ee.List.sequence(0, None, one.divide(nSplits), one.add(nSplits));\n",
        "\n",
        "    def f1(x):\n",
        "\n",
        "      def f2(y):\n",
        "        xp = one.subtract(x)\n",
        "        yp = one.subtract(y)\n",
        "        coeffs = ee.Array([[xp.multiply(yp), yp.multiply(x), xp.multiply(y), ee.Number(x).multiply(y)]])\n",
        "        return coeffs.matrixMultiply(rect).project([1])\n",
        "\n",
        "      return k.map(f2)\n",
        "\n",
        "    return k.map(f1).flatten()\n",
        "\n",
        "  # Return nSplit^2 polygons, given the list of vertices built by toPts().\n",
        "  def toRects(pts, nSplits):\n",
        "    offsets = ee.List([0, 1, one.add(nSplits).add(one), one.add(nSplits)])\n",
        "    k1 = ee.List.sequence(0, None, one.add(nSplits), nSplits)\n",
        "\n",
        "    def f3(i):\n",
        "      k2 = ee.List.sequence(i, None, 1, nSplits)\n",
        "\n",
        "      def f4(j):\n",
        "\n",
        "        def f5(offset):\n",
        "          return ee.Array(pts.get(ee.Number(j).add(offset))).toList()\n",
        "\n",
        "        return ee.Geometry.Polygon(offsets.map(f5))\n",
        "\n",
        "      return k2.map(f4)\n",
        "\n",
        "    return k1.map(f3).flatten()\n",
        "\n",
        "\n",
        "  # Get the 4 vertices.  Assumes that the scene geometry has only 4 vertices.\n",
        "  rect = ee.List(geom.coordinates().get(0))\n",
        "  rect = ee.Array([rect.get(0), rect.get(1), rect.get(3), rect.get(2)])\n",
        "\n",
        "  pts = toPts(rect, nSplits)\n",
        "  rects = toRects(pts, nSplits)\n",
        "\n",
        "  print('Parts that geometry was splitted to: ', ee.List(rects).size().getInfo())\n",
        "\n",
        "  return ee.List(rects)\n",
        "\n",
        "print(\"Splitting to tiles ...\")\n",
        "\n",
        "# get composite image bounds in a box\n",
        "i = mos.geometry().bounds()\n",
        "\n",
        "# split to SxS tiles\n",
        "splitted = split(i,S)\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s0L4H93mIKP"
      },
      "outputs": [],
      "source": [
        "# composite mosaic image\n",
        "image = mos79\n",
        "mountains = image.mask(kapos)\n",
        "\n",
        "def geom2feat(g):\n",
        "    return ee.Feature(ee.Geometry(g))\n",
        "features = splitted.map(geom2feat)\n",
        "splits = ee.FeatureCollection(features)\n",
        "\n",
        "tilesreduced = mountains.select('D0801B4').reduceRegions(**{\n",
        "  'collection': splits,\n",
        "  'reducer': ee.Reducer.mean(),\n",
        "  'scale': 200,\n",
        "})\n",
        "\n",
        "tilesMountainsIntersec = ee.FeatureCollection(tilesreduced.filter(ee.Filter.notNull(['mean'])))\n",
        "\n",
        "print('Tiles containing mountains: ', ee.FeatureCollection(tilesMountainsIntersec).size().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiE4EGADCv81",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# show on Map\n",
        "\n",
        "mapid = mountains.getMapId({'bands': ['D0801B4', 'D0801B3', 'D0801B2'], 'min': 300, 'max': 2500})\n",
        "map = folium.Map(location=[37.981892554434936, 23.7269115882649])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "# composite bounds\n",
        "# map.add_ee_layer(i, {}, 'composite bounds')\n",
        "\n",
        "no = 25\n",
        "lis = tilesMountainsIntersec.toList(ee.FeatureCollection(tilesMountainsIntersec).size().getInfo())\n",
        "spl = ee.Geometry(ee.List(lis.get(no)))\n",
        "desc = 'tile ' + str(int(no)) + ' from the split'\n",
        "\n",
        "# tile no\n",
        "map.add_ee_layer(spl, {}, desc)\n",
        "\n",
        "# # all tiles\n",
        "# map.add_ee_layer(splits, {}, \"splits\")\n",
        "\n",
        "# tiles with mountains\n",
        "map.add_ee_layer(tilesMountainsIntersec, {}, \"intersection\")\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNC20UFGMDST"
      },
      "source": [
        "# 5. Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "## Convert images to arrays\n",
        "\n",
        "Convert the imagery and GT into an array image in which each pixel stores ...x... patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGHYsdAOipa4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "features = ee.Image(masked_image.select(BANDS)).float()\n",
        "GT = ee.Image(gt.select(RESPONSE)).float()\n",
        "\n",
        "print('features: ', features.bandNames().getInfo())\n",
        "print('GT: ', GT.bandNames().getInfo())\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arraysFeatures = features.neighborhoodToArray(kernel)\n",
        "arraysGT = GT.neighborhoodToArray(kernel)\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "Take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the CNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export.\n",
        "For whole Greece takes around **1.5h** in GEE, and around 10GB with the current sample and shard sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0_m_XLK9ejQ"
      },
      "outputs": [],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "valPolysList = valPolys.toList(valPolys.size())\n",
        "testPolysList = testPolys.toList(testPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 2 # Number of shards in each polygon.\n",
        "N =  6 # Total sample size in each polygon. (N 256x256 patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "print(\"Creating sample patches and exporting ...\")\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "#  ---------- 1. Export features (imagery - X) ----------\n",
        "\n",
        "# # Export all the training data. -----------\n",
        "# # (in many pieces), with one task per geometry.\n",
        "\n",
        "# for g in range(trainingPolys.size().getInfo()):\n",
        "# # for g in range(46,trainingPolys.size().getInfo()):\n",
        "#     geomSample = ee.FeatureCollection([])\n",
        "#     for i in range(n):\n",
        "#         sample = arraysFeatures.sample(\n",
        "#             region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "#             scale = pixelSize,\n",
        "#             numPixels = N / n, # Size of the shard.\n",
        "#             seed = i,\n",
        "#             tileScale = 16\n",
        "#         )\n",
        "#         geomSample = geomSample.merge(sample)\n",
        "\n",
        "#     desc = 'X_' + TRAINING_BASE + '_g' + str(g)\n",
        "\n",
        "#     if (STORAGE == 'DRIVE'):\n",
        "#       task = ee.batch.Export.table.toDrive( ############ EXPORTS TO DRIVE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           folder = FOLDER,\n",
        "#           fileNamePrefix = desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = BANDS\n",
        "#       )\n",
        "#       task.start()\n",
        "#     else:\n",
        "#       task = ee.batch.Export.table.toCloudStorage(  ############ EXPORTS TO CLOUD STORAGE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           bucket = BUCKET,\n",
        "#           fileNamePrefix = FOLDER + '/' + desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = BANDS\n",
        "#       )\n",
        "#       task.start()\n",
        "\n",
        "\n",
        "\n",
        "# # Export all the validation data. ----------\n",
        "\n",
        "# for g in range(valPolys.size().getInfo()):\n",
        "# # for g in range(772,trainingPolys.size().getInfo()):\n",
        "#     geomSample = ee.FeatureCollection([])\n",
        "#     for i in range(n):\n",
        "#         sample = arraysFeatures.sample(\n",
        "#             region = ee.Feature(valPolysList.get(g)).geometry(),\n",
        "#             scale = pixelSize,\n",
        "#             numPixels = N / n,\n",
        "#             seed = i,\n",
        "#             tileScale = 16\n",
        "#         )\n",
        "#         geomSample = geomSample.merge(sample)\n",
        "\n",
        "#     desc = 'X_' + VAL_BASE + '_g' + str(g)\n",
        "\n",
        "#     if (STORAGE == 'DRIVE'):\n",
        "\n",
        "#       task = ee.batch.Export.table.toDrive( ############ EXPORTS TO DRIVE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           folder = FOLDER,\n",
        "#           fileNamePrefix = desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = BANDS\n",
        "#       )\n",
        "#       task.start()\n",
        "#     else:\n",
        "#       task = ee.batch.Export.table.toCloudStorage( ############ EXPORTS TO CLOUD STORAGE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           bucket = BUCKET,\n",
        "#           fileNamePrefix = FOLDER + '/' + desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = BANDS\n",
        "#       )\n",
        "#       task.start()\n",
        "\n",
        "\n",
        "# # Export all the test data. ----------\n",
        "\n",
        "# for g in range(testPolys.size().getInfo()):\n",
        "# # for g in range(202,testPolys.size().getInfo()):\n",
        "#     geomSample = ee.FeatureCollection([])\n",
        "#     for i in range(n):\n",
        "#         sample = arraysFeatures.sample(\n",
        "#             region = ee.Feature(testPolysList.get(g)).geometry(),\n",
        "#             scale = pixelSize,\n",
        "#             numPixels = N / n,\n",
        "#             seed = i,\n",
        "#             tileScale = 16\n",
        "#         )\n",
        "#         geomSample = geomSample.merge(sample)\n",
        "\n",
        "#     desc = 'X_' + TEST_BASE + '_g' + str(g)\n",
        "\n",
        "#     if (STORAGE == 'DRIVE'):\n",
        "#       task = ee.batch.Export.table.toDrive( ############ EXPORTS TO DRIVE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           folder = FOLDER,\n",
        "#           fileNamePrefix = desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = BANDS\n",
        "#       )\n",
        "#       task.start()\n",
        "#     else:\n",
        "#       task = ee.batch.Export.table.toCloudStorage( ############ EXPORTS TO CLOUD STORAGE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           bucket = BUCKET,\n",
        "#           fileNamePrefix = FOLDER + '/' + desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = BANDS\n",
        "#       )\n",
        "#       task.start()\n",
        "\n",
        "\n",
        "# print(\"----------\")\n",
        "# print(\"done exporting features (X_tra, X_val, X_test)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crkl_LrcL3VB"
      },
      "outputs": [],
      "source": [
        "print(\"Creating GT sample patches and exporting ...\")\n",
        "\n",
        "# #-----------------------------------------------------------------------\n",
        "# # ---------- 2. Export GT (ground truth - y) ----------\n",
        "\n",
        "# # Export all the training data. -----------\n",
        "\n",
        "# for g in range(trainingPolys.size().getInfo()):\n",
        "# # for g in range(844,trainingPolys.size().getInfo()):\n",
        "#     geomSample = ee.FeatureCollection([])\n",
        "#     for i in range(n):\n",
        "#         sample = arraysGT.sample(\n",
        "#             region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "#             scale = pixelSize,\n",
        "#             numPixels = N / n, # Size of the shard.\n",
        "#             seed = i,\n",
        "#             tileScale = 16\n",
        "#         )\n",
        "#         geomSample = geomSample.merge(sample)\n",
        "\n",
        "#     desc = 'y_' + TRAINING_BASE + '_g' + str(g)\n",
        "\n",
        "#     if (STORAGE == 'DRIVE'):\n",
        "#       task = ee.batch.Export.table.toDrive( ############ EXPORTS TO DRIVE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           folder = FOLDER,\n",
        "#           fileNamePrefix = desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = RESPONSE\n",
        "#       )\n",
        "#       task.start()\n",
        "#     else:\n",
        "#       task = ee.batch.Export.table.toCloudStorage(  ############ EXPORTS TO CLOUD STORAGE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           bucket = BUCKET,\n",
        "#           fileNamePrefix = FOLDER + '/' + desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = RESPONSE\n",
        "#       )\n",
        "#       task.start()\n",
        "\n",
        "# # Export all the validation data. ----------\n",
        "\n",
        "# for g in range(valPolys.size().getInfo()):\n",
        "# # for g in range(533,valPolys.size().getInfo()):\n",
        "# # because of error \"Too many tasks already in the queue (3000). Please wait for some of them to complete.\"\n",
        "\n",
        "#     geomSample = ee.FeatureCollection([])\n",
        "#     for i in range(n):\n",
        "#         sample = arraysGT.sample(\n",
        "#             region = ee.Feature(valPolysList.get(g)).geometry(),\n",
        "#             scale = pixelSize,\n",
        "#             numPixels = N / n, # Size of the shard.\n",
        "#             seed = i,\n",
        "#             tileScale = 16\n",
        "#         )\n",
        "#         geomSample = geomSample.merge(sample)\n",
        "\n",
        "#     desc = 'y_' + VAL_BASE + '_g' + str(g)\n",
        "\n",
        "#     if (STORAGE == 'DRIVE'):\n",
        "#       task = ee.batch.Export.table.toDrive( ############ EXPORTS TO DRIVE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           folder = FOLDER,\n",
        "#           fileNamePrefix = desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = RESPONSE\n",
        "#       )\n",
        "#       task.start()\n",
        "#     else:\n",
        "#       task = ee.batch.Export.table.toCloudStorage(  ############ EXPORTS TO CLOUD STORAGE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           bucket = BUCKET,\n",
        "#           fileNamePrefix = FOLDER + '/' + desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = RESPONSE\n",
        "#       )\n",
        "#       task.start()\n",
        "\n",
        "# # Export all the test data. ----------\n",
        "\n",
        "# for g in range(testPolys.size().getInfo()):\n",
        "# # for g in range(328,testPolys.size().getInfo()):\n",
        "# # because of error \"Too many tasks already in the queue (3000). Please wait for some of them to complete.\"\n",
        "\n",
        "#     geomSample = ee.FeatureCollection([])\n",
        "#     for i in range(n):\n",
        "#         sample = arraysGT.sample(\n",
        "#             region = ee.Feature(testPolysList.get(g)).geometry(),\n",
        "#             scale = pixelSize,\n",
        "#             numPixels = N / n, # Size of the shard.\n",
        "#             seed = i,\n",
        "#             tileScale = 16\n",
        "#         )\n",
        "#         geomSample = geomSample.merge(sample)\n",
        "\n",
        "#     desc = 'y_' + TEST_BASE + '_g' + str(g)\n",
        "\n",
        "#     if (STORAGE == 'DRIVE'):\n",
        "#       task = ee.batch.Export.table.toDrive( ############ EXPORTS TO DRIVE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           folder = FOLDER,\n",
        "#           fileNamePrefix = desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = RESPONSE\n",
        "#       )\n",
        "#       task.start()\n",
        "#     else:\n",
        "#       task = ee.batch.Export.table.toCloudStorage(  ############ EXPORTS TO CLOUD STORAGE ############\n",
        "#           collection = geomSample,\n",
        "#           description = desc,\n",
        "#           bucket = BUCKET,\n",
        "#           fileNamePrefix = FOLDER + '/' + desc,\n",
        "#           fileFormat = 'TFRecord',\n",
        "#           selectors = RESPONSE\n",
        "#       )\n",
        "#       task.start()\n",
        "\n",
        "# print(\"----------\")\n",
        "# print(\"done exporting ground truth (y_tra, y_val, y_test)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# 6. Load sample Data\n",
        "\n",
        "Download data from Google Drive and load the data exported from Earth Engine into a `tf.data.Dataset`.\n",
        "Loading takes around **30m**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGRJeu4Cv9O"
      },
      "source": [
        "### Helper functions for reading training, validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "def to_single(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "def to_multichannel(inputs):\n",
        "    # inputsList = [inputs.get(key) for key in RESPONSE_DICT.keys()]\n",
        "    # dim = tf.stack(inputsList, axis=0)\n",
        "    dim = inputs.get(RESPONSE[0])\n",
        "    label_channels = []    # or maybe a dict?\n",
        "    for ch in UNIQUES:    # iterate through each desired label of pixels (found above through the unique values in the images)\n",
        "        label_channels.append( (dim==ch).astype(tf.int32) )    # create a channel for each label and store in a list\n",
        "    stacked_label = tf.transpose(tf.stack(label_channels), [1, 2, 0])    # stack them in a single\n",
        "    return stacked_label\n",
        "\n",
        "def get_labeled_dataset(prefix, driveTrue):\n",
        "\n",
        "  if (STORAGE == 'DRIVE'): # if samples are stored in Drive\n",
        "    pattern = BASE_FOLDER + FOLDER + '/' + '*'\n",
        "  else: # if samples are stored in Google Cloud\n",
        "    pattern =  'gs://' + BUCKET + '/' + FOLDER + '/' + '*'\n",
        "\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "\n",
        "  x_glob = [x for x in glob if x.split(\"/\")[-1].split(\"_\")[0]==\"X\" and x.split(\"/\")[-1].split(\"_\")[1]==prefix]\n",
        "  x_dataset = tf.data.TFRecordDataset(x_glob, compression_type='GZIP')\n",
        "  x_dataset = x_dataset.map(\n",
        "        lambda x: tf.io.parse_single_example(x, features=BANDS_DICT),\n",
        "        num_parallel_calls=5\n",
        "  )\n",
        "  x_dataset = x_dataset.map(to_single, num_parallel_calls=5)\n",
        "\n",
        "  y_glob = [x for x in glob if x.split(\"/\")[-1].split(\"_\")[0]==\"y\" and x.split(\"/\")[-1].split(\"_\")[1]==prefix]\n",
        "  y_dataset = tf.data.TFRecordDataset(y_glob, compression_type='GZIP')\n",
        "  y_dataset = y_dataset.map(\n",
        "        lambda x: tf.io.parse_single_example(x, features=RESPONSE_DICT),\n",
        "        num_parallel_calls=5\n",
        "  )\n",
        "  y_dataset = y_dataset.map(to_multichannel, num_parallel_calls=5)\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\n",
        "\n",
        "  if (prefix == \"tra\"):\n",
        "    dataset = dataset.map(\n",
        "                            lambda image, label: (tf.image.flip_left_right(image), label)\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.flip_up_down(image), label)\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.rot90(image), label) # rotate 90 degr\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.rot90(image, k=-2), label) # rotate 180 degr\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.rot90(image, k=3), label) # rotate 270 degr\n",
        "                      ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "  elif (prefix == \"val\"):\n",
        "    dataset = dataset.map(\n",
        "                            lambda image, label: (tf.image.flip_left_right(image), label)\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.flip_up_down(image), label)\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.rot90(image), label) # rotate 90 degr\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.rot90(image, k=-2), label) # rotate 180 degr\n",
        "                      ).map(\n",
        "                            lambda image, label: (tf.image.rot90(image, k=3), label) # rotate 270 degr\n",
        "                      ).batch(1).repeat()\n",
        "  elif (prefix == \"test\"):\n",
        "    dataset = dataset.batch(1).repeat()\n",
        "\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "### Training data\n",
        "\n",
        "Use the helpers to read in the training dataset.  Print the first record to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "training = get_labeled_dataset(\"tra\", False)\n",
        "\n",
        "print(\"Inspecting training dataset's dimensions: \")\n",
        "for x in training.take(1):\n",
        "    print(x[0].numpy().shape)\n",
        "    print(x[1].numpy().shape)\n",
        "\n",
        "print(training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "### Validation and Test data\n",
        "\n",
        "Now do the same thing to get an validation and test dataset.  Note that unlike the training dataset, the validation and test dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "validation = get_labeled_dataset(\"val\", False)\n",
        "\n",
        "print(\"Inspecting validation dataset's dimensions: \")\n",
        "for x in validation.take(1):\n",
        "    print(x[0].numpy().shape)\n",
        "    print(x[1].numpy().shape)\n",
        "\n",
        "print(validation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5iMNuP59jGs"
      },
      "outputs": [],
      "source": [
        "test = get_labeled_dataset(\"test\", False)\n",
        "\n",
        "print(\"Inspecting test dataset's dimensions: \")\n",
        "for x in test.take(1):\n",
        "    print(x[0].numpy().shape)\n",
        "    print(x[1].numpy().shape)\n",
        "\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# 7. Existing model definition\n",
        "Model acquired from Stoian et al., 2019, https://www.mdpi.com/2072-4292/11/17/1986\n",
        "here: https://github.com/vpoughon/RT_DL_OSO_public"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOoeKGw5Cv9g"
      },
      "source": [
        "### Model\n",
        "\"get_unet_mlp_ts\" model from here: https://github.com/vpoughon/RT_DL_OSO_public/blob/master/model_definitions.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "832BWKsLPLcW"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Concatenate, core, Dropout, concatenate, Cropping2D, Convolution2D, ConvLSTM2D, Conv3D\n",
        "\n",
        "def get_unet_mlp_ts(n_ch, n_timesteps, n_output, patch_height, patch_width,hidden_layers):\n",
        "# create u-net model\n",
        "\n",
        "    inputs = Input((patch_height, patch_width, n_ch))\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    #\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    #\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Dropout(0.2)(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "\n",
        "    up1 = concatenate([UpSampling2D(size=(2, 2))(conv3), conv2], axis=-1)\n",
        "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1)\n",
        "    conv4 = Dropout(0.2)(conv4)\n",
        "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    #\n",
        "    up2 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv1], axis=-1)\n",
        "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "    conv5 = Dropout(0.2)(conv5)\n",
        "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    unet_model = Model(inputs=inputs, outputs=conv5)\n",
        "    #\n",
        "\n",
        "    ###\n",
        "\n",
        "    l_prev = Conv2D(hidden_layers[0], (1, 1), activation='relu', padding='same')(inputs)\n",
        "    for n_hidden_neurons in hidden_layers[1:]:\n",
        "        fc_new = Conv2D(n_hidden_neurons, (1, 1), activation='relu', padding='same')(l_prev)\n",
        "        # added by VP to decrease overfitting:\n",
        "        fc_new = Dropout(0.2)(fc_new)\n",
        "        l_prev = fc_new\n",
        "    mlp_model = Model(inputs=inputs, outputs=l_prev)\n",
        "\n",
        "    # share this model among temporal samples\n",
        "    inputs_list = []\n",
        "    out_list = []\n",
        "    out_mlp_list = []\n",
        "    for i in range(n_timesteps):\n",
        "        input_patch = Input((patch_height, patch_width, n_ch))\n",
        "        inputs_list.append(input_patch)\n",
        "        out_unet_layer = unet_model(input_patch)\n",
        "        out_mlp_layer = mlp_model(input_patch)\n",
        "        out_list.append(out_unet_layer)\n",
        "        out_mlp_list.append(out_mlp_layer)\n",
        "\n",
        "    out_unet = concatenate(out_list,axis=-1)\n",
        "    out_mlp = concatenate(out_mlp_list,axis=-1)\n",
        "\n",
        "    out_concat = concatenate([out_mlp, out_unet],axis=-1)\n",
        "    out_concat = core.Reshape(((patch_height,patch_width, hidden_layers[-1]+32)*n_timesteps))(out_concat)\n",
        "\n",
        "    # create common layers for softmax\n",
        "    out2 = Conv2D(n_output, (1, 1), activation='relu',padding='same')(out_concat)\n",
        "    # !!!! KEEP THIS RESHAPE FOR LOSS IN TRAINING !!!!\n",
        "    # !!!! COMMENT OUT AND COMPILE AGAIN FOR PREDICTION !!!!\n",
        "    out2 = core.Reshape((patch_height*patch_width, n_output))(out2)\n",
        "    out2 = core.Activation('softmax')(out2)\n",
        "\n",
        "    # get unified model\n",
        "    model_out = Model(inputs_list,out2)\n",
        "\n",
        "    return model_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuB8O08gQdgd"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aslO7Zb0Qe0s"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Reshape, core\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "\n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "\n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-NLeO3alYrK"
      },
      "outputs": [],
      "source": [
        "# ******************** LOSS ***********************\n",
        "\n",
        "if weights is not None:\n",
        "\t\tLOSS = weighted_categorical_crossentropy(weights)\n",
        "else:\n",
        "\t\tLOSS='categorical_crossentropy'\n",
        "\n",
        "\n",
        "# ---------- DICE LOSS ----------\n",
        "# dice_loss = sm.losses.DiceLoss()\n",
        "# LOSS = dice_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maWHFL_ymDP-"
      },
      "source": [
        "### Accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONhjI06ImCuh"
      },
      "outputs": [],
      "source": [
        "# # -----------------------------------------\n",
        "\n",
        "# iou = sm.metrics.IOUScore(threshold=0.5)\n",
        "# fscore = sm.metrics.FScore(threshold=0.5)\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# -----------------------------------------\n",
        "# Stoian et al. ----->\n",
        "# 1\n",
        "def acc_labeled_only(y_true, y_pred):\n",
        "    '''\n",
        "    accuracy evaluated only on labeled pixels\n",
        "    '''\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    annot_mask = K.cast(K.not_equal(K.max(y_true, axis=-1), 0), 'int64')\n",
        "    num_annot = K.sum(annot_mask)\n",
        "    num_not_annot = K.sum(1 - annot_mask)\n",
        "    y_true_label = K.argmax(y_true, axis=-1)\n",
        "    y_pred_label = K.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # all not annotated GT will be 0\n",
        "    y_pred_all = (y_pred_label + K.ones_like(y_pred_label)) * annot_mask\n",
        "    # all not annotated PRED will be 0, first valid class=1\n",
        "    y_true_all = (y_true_label + K.ones_like(y_true_label)) * annot_mask\n",
        "\n",
        "    # 0 = classes do not match, else 1. will be 1 for non annotated pixels\n",
        "    mask_class_ok = K.cast(K.equal(y_pred_all, y_true_all), 'int64')\n",
        "\n",
        "    # count all matching, subtract non annotated, divide by number of annotated\n",
        "    return K.cast(K.sum(mask_class_ok) - num_not_annot, 'float32') / K.cast(num_annot, 'float32')\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "METRICS = [acc_labeled_only]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n71C7a6hCv9r"
      },
      "source": [
        "\n",
        "## Build model (compile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9RqVKbTEphq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"compiling model...\")\n",
        "\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(lr) # (Stoian, Schmitt, Scepanovic, Hua)\n",
        "# OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
        "\n",
        "# ********************* function to initialize model ********************\n",
        "\n",
        "def get_model():\n",
        "\t# get_unet_mlp_ts(n_ch, n_timesteps, n_output, patch_height, patch_width, hidden_layers)\n",
        "\tm = get_unet_mlp_ts(len(BANDS),1,CLASS_NUMBER,KERNEL_SIZE,KERNEL_SIZE,[200,100,50])\n",
        "\tm.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "\treturn m\n",
        "\n",
        "m_stoian = get_model()\n",
        "m_stoian.summary()\n",
        "\n",
        "model = m_stoian\n",
        "\n",
        "print(\"...done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDbF5xkPBmH"
      },
      "source": [
        "# 8. Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLF6lzrHkqy9"
      },
      "source": [
        "## Set callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv6Ox4BH-7E-"
      },
      "outputs": [],
      "source": [
        "# path for best model\n",
        "filename = 'best_model-STOIAN-epoch{epoch:02d}-loss{loss:.4f}-vloss{val_loss:.4f}-acc{acc_labeled_only:.4f}-vacc{val_acc_labeled_only:.4f}'\n",
        "\n",
        "if (STORAGE == 'DRIVE'):\n",
        "  bpath = BASE_FOLDER + FOLDER + '/' + filename\n",
        "else:\n",
        "  bpath =  'gs://' + BUCKET + '/' + FOLDER + '/' + filename\n",
        "\n",
        "# ---------------------- CALLBACKS --------------------------\n",
        "\n",
        "# best model is saved in every checkpoint\n",
        "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    filepath=bpath,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)\n",
        "\n",
        "# early stopping\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    verbose=1,\n",
        "    patience=5, # wait for 5 epochs\n",
        ")\n",
        "\n",
        "# # LR reducer\n",
        "# from keras.callbacks import ReduceLROnPlateau\n",
        "# lr_reducer = ReduceLROnPlateau(\n",
        "#     monitor='val_loss',\n",
        "#     factor=0.9,\n",
        "#     cooldown=0,\n",
        "#     patience=5,\n",
        "#     min_lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7NpQmYGTQF"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZaw7Y_a86Ri"
      },
      "outputs": [],
      "source": [
        "# ---------------------- TRAIN --------------------------\n",
        "\n",
        "print(\"training model...\")\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "history=model.fit(\n",
        "          x=training,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=validation,\n",
        "          validation_steps=VAL_SIZE,\n",
        "          # callbacks=[lr_reducer, save_model, stopping],\n",
        "          callbacks=[save_model, stopping],\n",
        "          steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE)\n",
        "          )\n",
        "\n",
        "# save history\n",
        "hfilename = 'trainHistoryDict'\n",
        "\n",
        "if (STORAGE == 'DRIVE'):\n",
        "  hpath = BASE_FOLDER + FOLDER + '/' + hfilename\n",
        "  with open(hpath, 'wb') as file_pi:\n",
        "    pickle.dump(history, file_pi)\n",
        "else:\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(BUCKET) # bucket_name = \"your-bucket-name\"\n",
        "  blob = bucket.blob('/' + FOLDER + '/' + hfilename) # blob_name = \"storage-object-name\"\n",
        "  with blob.open(\"wb\") as f: # Mode can be specified as wb/rb for bytes mode.\n",
        "      pickle.dump(history, f)\n",
        "\n",
        "\n",
        "duration = datetime.now()-start\n",
        "print(\"...done!\")\n",
        "print(\"Training completed in time: \", duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFFoh98u-7Y"
      },
      "source": [
        "## Load a model from latest checkpoint and continue training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8o5Qcqj2GJf",
        "outputId": "86b53ca8-7a14-4a62-9821-64feb73fbf68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from trained model...\n",
            "...done!\n"
          ]
        }
      ],
      "source": [
        "print(\"loading weights from trained model...\")\n",
        "\n",
        "# m_updated = get_model()\n",
        "\n",
        "# get the latest (best) model saved\n",
        "\n",
        "if (STORAGE == 'DRIVE'):\n",
        "  path = BASE_FOLDER + FOLDER\n",
        "else:\n",
        "  path =  'gs://' + BUCKET + '/' + FOLDER\n",
        "\n",
        "latest = tf.train.latest_checkpoint(path)\n",
        "model.load_weights(latest)\n",
        "\n",
        "print(\"...done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RoRT1L-u_Ud"
      },
      "outputs": [],
      "source": [
        "EPOCHS_LEFT = 45\n",
        "\n",
        "# resume the training where we left off\n",
        "start = datetime.now()\n",
        "\n",
        "history=model.fit(\n",
        "          x=training,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS_LEFT,\n",
        "          verbose=1,\n",
        "          validation_data=validation,\n",
        "          validation_steps=VAL_SIZE,\n",
        "          # callbacks=[lr_reducer, save_model, stopping],\n",
        "          callbacks=[save_model, stopping],\n",
        "          steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE)\n",
        "          )\n",
        "\n",
        "# save history\n",
        "hfilename = 'trainHistoryDict'\n",
        "\n",
        "if (STORAGE == 'DRIVE'):\n",
        "  hpath = BASE_FOLDER + FOLDER + '/' + hfilename\n",
        "  with open(hpath, 'wb') as file_pi:\n",
        "    pickle.dump(history, file_pi)\n",
        "else:\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(BUCKET) # bucket_name = \"your-bucket-name\"\n",
        "  blob = bucket.blob('/' + FOLDER + '/' + hfilename) # blob_name = \"storage-object-name\"\n",
        "  with blob.open(\"wb\") as f: # Mode can be specified as wb/rb for bytes mode.\n",
        "      pickle.dump(history, f)\n",
        "\n",
        "\n",
        "duration = datetime.now()-start\n",
        "print(\"...done!\")\n",
        "print(\"Training completed in time: \", duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd5j61S2EI9V"
      },
      "source": [
        "# 9. Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK6P9tm7doFI"
      },
      "source": [
        "## Load a model from latest checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YODvOOdMdoFM"
      },
      "outputs": [],
      "source": [
        "print(\"loading weights from trained model...\")\n",
        "\n",
        "# m_updated = get_model()\n",
        "\n",
        "# get the latest (best) model saved\n",
        "\n",
        "if (STORAGE == 'DRIVE'):\n",
        "  path = BASE_FOLDER + FOLDER\n",
        "else:\n",
        "  path =  'gs://' + BUCKET + '/' + FOLDER\n",
        "\n",
        "latest = tf.train.latest_checkpoint(path)\n",
        "model.load_weights(latest)\n",
        "\n",
        "print(\"...done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgUkuEayG5Pz"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuySuyt0EKtM"
      },
      "outputs": [],
      "source": [
        "from classification_models import models\n",
        "import keras.backend as K\n",
        "\n",
        "EVAL_METRICS = [f1_m, recall_m, precision_m]\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model.compile(keras.optimizers.Adam(lr), LOSS, EVAL_METRICS)\n",
        "\n",
        "# evaluate the model\n",
        "\n",
        "print(\"evaluating model...\")\n",
        "\n",
        "loss, f1_score, recall, precision = model.evaluate(x=test, steps=int(TEST_SIZE / BATCH_SIZE))\n",
        "\n",
        "print('...done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# 10. Prediction\n",
        "\n",
        "The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storge bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebWDP3Qez-7"
      },
      "source": [
        "## Model modified for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BxXfxAjez_A"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, Concatenate, core, Dropout, concatenate, Cropping2D, Convolution2D, ConvLSTM2D, Conv3D\n",
        "\n",
        "def get_unet_mlp_ts(n_ch, n_timesteps, n_output, patch_height, patch_width,hidden_layers):\n",
        "# create u-net model\n",
        "\n",
        "    inputs = Input((patch_height, patch_width, n_ch))\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    #\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    #\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Dropout(0.2)(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "\n",
        "    up1 = concatenate([UpSampling2D(size=(2, 2))(conv3), conv2], axis=-1)\n",
        "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1)\n",
        "    conv4 = Dropout(0.2)(conv4)\n",
        "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    #\n",
        "    up2 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv1], axis=-1)\n",
        "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "    conv5 = Dropout(0.2)(conv5)\n",
        "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    unet_model = Model(inputs=inputs, outputs=conv5)\n",
        "    #\n",
        "\n",
        "    ###\n",
        "\n",
        "    l_prev = Conv2D(hidden_layers[0], (1, 1), activation='relu', padding='same')(inputs)\n",
        "    for n_hidden_neurons in hidden_layers[1:]:\n",
        "        fc_new = Conv2D(n_hidden_neurons, (1, 1), activation='relu', padding='same')(l_prev)\n",
        "        # added by VP to decrease overfitting:\n",
        "        fc_new = Dropout(0.2)(fc_new)\n",
        "        l_prev = fc_new\n",
        "    mlp_model = Model(inputs=inputs, outputs=l_prev)\n",
        "\n",
        "    # share this model among temporal samples\n",
        "    inputs_list = []\n",
        "    out_list = []\n",
        "    out_mlp_list = []\n",
        "    for i in range(n_timesteps):\n",
        "        input_patch = Input((patch_height, patch_width, n_ch))\n",
        "        inputs_list.append(input_patch)\n",
        "        out_unet_layer = unet_model(input_patch)\n",
        "        out_mlp_layer = mlp_model(input_patch)\n",
        "        out_list.append(out_unet_layer)\n",
        "        out_mlp_list.append(out_mlp_layer)\n",
        "\n",
        "    out_unet = concatenate(out_list,axis=-1)\n",
        "    out_mlp = concatenate(out_mlp_list,axis=-1)\n",
        "\n",
        "    out_concat = concatenate([out_mlp, out_unet],axis=-1)\n",
        "    out_concat = core.Reshape(((patch_height,patch_width, hidden_layers[-1]+32)*n_timesteps))(out_concat)\n",
        "\n",
        "    # create common layers for softmax\n",
        "    out2 = Conv2D(n_output, (1, 1), activation='relu',padding='same')(out_concat)\n",
        "    # !!!! KEEP THIS RESHAPE FOR LOSS IN TRAINING !!!!\n",
        "    # !!!! COMMENT OUT AND COMPILE AGAIN FOR PREDICTION !!!!\n",
        "    # out2 = core.Reshape((patch_height*patch_width, n_output))(out2)\n",
        "    # out2 = core.Activation('softmax')(out2)\n",
        "\n",
        "    # get unified model\n",
        "    model_out = Model(inputs_list,out2)\n",
        "\n",
        "    return model_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA-tx_Dd9Otc"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyAgtUmT9Ote"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Reshape, core\n",
        "\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "\n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "\n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA7DxlTY9Oti"
      },
      "outputs": [],
      "source": [
        "# ******************** LOSS ***********************\n",
        "\n",
        "if weights is not None:\n",
        "\t\tLOSS = weighted_categorical_crossentropy(weights)\n",
        "else:\n",
        "\t\tLOSS='categorical_crossentropy'\n",
        "\n",
        "\n",
        "# ---------- DICE LOSS ----------\n",
        "# dice_loss = sm.losses.DiceLoss()\n",
        "# LOSS = dice_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80uqxKmN9Otk"
      },
      "source": [
        "### Accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P47QHHfL9Otl"
      },
      "outputs": [],
      "source": [
        "# # -----------------------------------------\n",
        "\n",
        "# iou = sm.metrics.IOUScore(threshold=0.5)\n",
        "# fscore = sm.metrics.FScore(threshold=0.5)\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# -----------------------------------------\n",
        "# Stoian et al. ----->\n",
        "# 1\n",
        "def acc_labeled_only(y_true, y_pred):\n",
        "    '''\n",
        "    accuracy evaluated only on labeled pixels\n",
        "    '''\n",
        "    y_true = core.Reshape((KERNEL_SIZE*KERNEL_SIZE, CLASS_NUMBER))(y_true)\n",
        "\n",
        "    annot_mask = K.cast(K.not_equal(K.max(y_true, axis=-1), 0), 'int64')\n",
        "    num_annot = K.sum(annot_mask)\n",
        "    num_not_annot = K.sum(1 - annot_mask)\n",
        "    y_true_label = K.argmax(y_true, axis=-1)\n",
        "    y_pred_label = K.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # all not annotated GT will be 0\n",
        "    y_pred_all = (y_pred_label + K.ones_like(y_pred_label)) * annot_mask\n",
        "    # all not annotated PRED will be 0, first valid class=1\n",
        "    y_true_all = (y_true_label + K.ones_like(y_true_label)) * annot_mask\n",
        "\n",
        "    # 0 = classes do not match, else 1. will be 1 for non annotated pixels\n",
        "    mask_class_ok = K.cast(K.equal(y_pred_all, y_true_all), 'int64')\n",
        "\n",
        "    # count all matching, subtract non annotated, divide by number of annotated\n",
        "    return K.cast(K.sum(mask_class_ok) - num_not_annot, 'float32') / K.cast(num_annot, 'float32')\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "METRICS = [acc_labeled_only]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwCOxrjR9IA1"
      },
      "source": [
        "\n",
        "## Build model (compile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqDz69NX9IA4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"compiling model...\")\n",
        "\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(lr) # (Stoian, Schmitt, Scepanovic, Hua)\n",
        "# OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
        "\n",
        "# ********************* function to initialize model ********************\n",
        "\n",
        "def get_model():\n",
        "\t# get_unet_mlp_ts(n_ch, n_timesteps, n_output, patch_height, patch_width, hidden_layers)\n",
        "\tm = get_unet_mlp_ts(len(BANDS),1,CLASS_NUMBER,KERNEL_SIZE,KERNEL_SIZE,[200,100,50])\n",
        "\tm.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "\treturn m\n",
        "\n",
        "m_stoian = get_model()\n",
        "m_stoian.summary()\n",
        "\n",
        "model = m_stoian\n",
        "\n",
        "print(\"...done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxaXIdNCez_E"
      },
      "source": [
        "## Load a model from latest checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0MYo85Tez_F"
      },
      "outputs": [],
      "source": [
        "print(\"loading weights from trained model...\")\n",
        "\n",
        "# m_updated = get_model()\n",
        "\n",
        "# get the latest (best) model saved\n",
        "\n",
        "if (STORAGE == 'DRIVE'):\n",
        "  path = BASE_FOLDER + FOLDER\n",
        "else:\n",
        "  path =  'gs://' + BUCKET + '/' + FOLDER\n",
        "\n",
        "latest = tf.train.latest_checkpoint(path)\n",
        "model.load_weights(latest)\n",
        "\n",
        "print(\"...done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Ndc1VdX6wR"
      },
      "source": [
        "## Set variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPANwc7B1-TS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "user_folder = 'users/n-verde' # INSERT YOUR GEE FOLDER HERE.\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "image_base = '1542_CLC_pred_'\n",
        "\n",
        "# Half this will extend on the sides of each patch.\n",
        "kernel_buffer = [0, 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qroQnAOYGeB"
      },
      "outputs": [],
      "source": [
        "# mask image to Kapos mountain area\n",
        "# image4pred = image_res.mask(kapos)\n",
        "image4pred = image_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGu-oAHH7cq"
      },
      "source": [
        "## Export image for prediction\n",
        "as TFRecord in Google Cloud. Takes approx **5-13m** per tile (all Greece it takes around **30m**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region, tileNum):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image4pred.select(BANDS),\n",
        "    description = out_image_base + str(tileNum) + '_',\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + '/' + out_image_base + str(tileNum) + '_',\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = pixelSize,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "\n",
        "#   # Block until the task completes.\n",
        "#   print('Running image export to Google Drive...')\n",
        "#   import time\n",
        "#   while task.active():\n",
        "#     time.sleep(30)\n",
        "\n",
        "#   # Error condition\n",
        "#   if task.status()['state'] != 'COMPLETED':\n",
        "#     print('Error with image export.')\n",
        "#   else:\n",
        "#     print('Image export completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ6CP0kY--RL"
      },
      "outputs": [],
      "source": [
        "# Run the export for tiles of Greece\n",
        "\n",
        "# A) loop\n",
        "tile_number = int(0) # 1st tile\n",
        "for i in range(0,(len(lis.getInfo()))):\n",
        "\n",
        "# # B) if you want to run for specific tiles\n",
        "# tilesList = [149,150,151,745,746]\n",
        "# for i in tilesList: #\n",
        "\n",
        "  tile_number = int(i)\n",
        "\n",
        "  # # option a. without buffer\n",
        "  # tile = ee.Feature(ee.List(lis.get(i))).geometry().getInfo()\n",
        "  # region = tile[\"coordinates\"]\n",
        "  # # print(region)\n",
        "\n",
        "  # option b. with buffer\n",
        "  # buffer the tile 2560m to avoid data gaps between tiles\n",
        "  # the distance of 2560m was found empirically by running the code and seeing the gaps\n",
        "  prj = ee.Projection('EPSG:3035');  # European projection\n",
        "  buffer = ee.Feature(ee.List(lis.get(i))).geometry().buffer(2560,None,prj).bounds(0.1)\n",
        "  buffered_tile = buffer.getInfo()\n",
        "  buffered_region = buffered_tile[\"coordinates\"]\n",
        "  region = buffered_region\n",
        "  # print(region)\n",
        "\n",
        "  tile_region = ee.Geometry.Polygon(region, None, False)\n",
        "\n",
        "  # convert \"1\" to \"01\"\n",
        "  tile_number_string = str(tile_number).zfill(3)\n",
        "\n",
        "  doExport(image_base, kernel_buffer, tile_region, tile_number_string)\n",
        "\n",
        "  print('done exporting for tile: ', tile_number_string, region)\n",
        "  tile_number = tile_number\n",
        "\n",
        "  print('OK!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeK6vPauPw4g"
      },
      "source": [
        "## Run prediction\n",
        "Run the prediction and upload to GEE (for each tile (around 50 tiles), *prediction* takes **~3m** and *upload* to GEE takes around **2h**).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def doPrediction(out_image_base, user_folder, kernel_buffer, region):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  # __________________________________\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = !gsutil ls 'gs://'{BUCKET}'/'{FOLDER}\n",
        "  # Get only the files generated by the image export for the specific tile.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  jsonText = !gsutil cat {jsonFile}\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # __________________________________\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "\n",
        "  patch_width = KERNEL_SHAPE[0] + kernel_buffer[0]\n",
        "  patch_height = KERNEL_SHAPE[1] + kernel_buffer[1]\n",
        "\n",
        "  buffered_shape = [\n",
        "      patch_width,\n",
        "      patch_height]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # __________________________________\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = model.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions)\n",
        "\n",
        "  # # WRITE PREDICTIONS TO STORAGE\n",
        "  # storage_client = storage.Client()\n",
        "  # bucket = storage_client.bucket(BUCKET) # bucket_name = \"your-bucket-name\"\n",
        "  # blob = bucket.blob(FOLDER + '/' + 'predictions.p') # blob_name = \"storage-object-name\"\n",
        "  # with blob.open(\"wb\") as f: # Mode can be specified as wb/rb for bytes mode.\n",
        "  #     pickle.dump(predictions, f)\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + '/' + out_image_base + '.TFRecord'\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "\n",
        "  # __________________________________\n",
        "\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "    # print(predictionPatch.shape) # (256, 256, 6)\n",
        "\n",
        "    # predictionPatch = np.reshape(predictionPatch,(KERNEL_SIZE,KERNEL_SIZE,predictionPatch.shape[-1]))\n",
        "    # print(predictionPatch.shape) # (256, 256, 6)\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "          feature={\n",
        "            'pred': tf.train.Feature(\n",
        "                float_list=tf.train.FloatList(\n",
        "                    value=np.argmax(predictionPatch, axis=2).flatten())),\n",
        "          }\n",
        "        )\n",
        "      )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "  # Start the upload.\n",
        "  out_image_asset = user_folder + '/' + out_image_base\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "\n",
        "  print('DONE!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Run the prediction for tiles of Greece\n",
        "\n",
        "# just to invoke earth engine so it doesn't disconnect\n",
        "try:\n",
        "  import ee\n",
        "  ee.Initialize()\n",
        "except:\n",
        "  import ee\n",
        "  ee.Authenticate()\n",
        "  ee.Initialize()\n",
        "\n",
        "\n",
        "# A) loop\n",
        "tile_number = int(0) # 1st tile\n",
        "for i in range(0,(len(lis.getInfo()))):\n",
        "\n",
        "# # B) if you want to run for specific tiles\n",
        "# tilesList = [149,150,151,745,746]\n",
        "# for i in tilesList: #\n",
        "\n",
        "  tile_number = int(i)\n",
        "\n",
        "  # # option a. without buffer\n",
        "  # tile = ee.Feature(ee.List(lis.get(i))).geometry().getInfo()\n",
        "  # region = tile[\"coordinates\"]\n",
        "  # # print(region)\n",
        "\n",
        "  # option b. with buffer\n",
        "  # buffer the tile 2560m to avoid data gaps between tiles\n",
        "  # the distance of 2560m was found empirically by running the code and seeing the gaps\n",
        "  prj = ee.Projection('EPSG:3035');  # European projection\n",
        "  buffer = ee.Feature(ee.List(lis.get(i))).geometry().buffer(2560,None,prj).bounds(0.1)\n",
        "  buffered_tile = buffer.getInfo()\n",
        "  buffered_region = buffered_tile[\"coordinates\"]\n",
        "  region = buffered_region\n",
        "  # print(region)\n",
        "\n",
        "  tile_region = ee.Geometry.Polygon(region, None, False)\n",
        "\n",
        "  # convert \"1\" to \"01\"\n",
        "  tile_number_string = str(tile_number).zfill(3)\n",
        "\n",
        "  image_base2 = image_base + str(tile_number_string)\n",
        "\n",
        "  doPrediction(image_base2, user_folder, kernel_buffer, tile_region)\n",
        "\n",
        "  print('done exporting for tile: ', i, region)\n",
        "  tile_number = tile_number\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}